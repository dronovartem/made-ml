{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Импортируем необходимые библиотеки, а также напишем пару функций для обработки данных:\n",
    "- tf_binarize: фактически LabelEncoder, ставит 1 вместо `t`, 0 вместо `t`;\n",
    "- quantile_bucketing: группирует хвост распределения заданного веса;\n",
    "- exist_binarize: ставит 1, если столбец содержит данные, 0 иначе;\n",
    "- date_transform: переводит время в количество секунд с начала отсчета;"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "\n",
    "# Handmade transformers\n",
    "def tf_binarize(col):\n",
    "    return np.where(col == 't', 1, 0)\n",
    "\n",
    "def quantile_bucketing(col, level = 0.95):\n",
    "    q = col.quantile(level)\n",
    "    return np.where(col > q, q, col)\n",
    "\n",
    "def exist_binarize(col):\n",
    "    return np.where(col.isna(), 0, 1)\n",
    "\n",
    "def date_transform(col):\n",
    "    return list(map(lambda x: (x - datetime.datetime(1970, 1, 1)).total_seconds(), col))\n",
    "\n",
    "\n",
    "# custom metric\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "source": [
    "Для начала считаем и объединим все данные, чтобы подобрать оценить разбросы фичей."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(74815, 43)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "df = pd.concat([train, test])\n",
    "del train, test\n",
    "df.shape"
   ]
  },
  {
   "source": [
    "## 1. Extra data/features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1 Calendar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Из календаря извлечем количество дней, в которые был доступен данный лот."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('calendar.csv')\n",
    "calendar.available = tf_binarize(calendar.available)\n",
    "calendar.rename(columns = {'listing_id' : 'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = '2019-11-04' \n",
    "year_ago_date = '2018-11-04'\n",
    "\n",
    "av_year = calendar[calendar.date > year_ago_date].groupby('id')['available'].sum()\n",
    "av_year = av_year.reset_index()\n",
    "\n",
    "av_year.rename(columns = {'available' : 'available_year'}, inplace=True)\n",
    "del calendar"
   ]
  },
  {
   "source": [
    "## 1.2 Reviews"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Из отзывов возьмем количество уникальных отзывов о лоте за все время."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.csv')\n",
    "reviews = reviews[~reviews.comments.isna()]\n",
    "\n",
    "unique_comments = reviews.groupby('listing_id')['reviewer_id'].nunique().reset_index()\n",
    "unique_comments.rename(columns = {'reviewer_id' : 'unique_reviewers', 'listing_id' : 'id'}, inplace=True)\n",
    "del reviews"
   ]
  },
  {
   "source": [
    "## 1.3 Host owns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Данная фича показывает, сколько объектов владельца лота выставлено для аренды."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_owns = df.groupby('host_id')['id'].count().reset_index()\n",
    "host_owns.rename(columns = {'id' : 'host_realty_count'}, inplace=True)\n",
    "#host_owns.host_realty_count = quantile_bucketing(host_owns.host_realty_count, 0.99)"
   ]
  },
  {
   "source": [
    "## 2. Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Для удобства все фичи разбиты по спискам, с указанием последующего преобразования для них."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['price']\n",
    "\n",
    "looks_fine_features = [\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'bathrooms',\n",
    "    'minimum_nights',\n",
    "    'host_since',\n",
    "]\n",
    "\n",
    "drop_features = [\n",
    "    'id', # no way\n",
    "    'name', # looks noisy\n",
    "    'summary', 'space', 'description', # aren't they always positive?\n",
    "    'access', 'interaction', 'house_rules', # lot of empties and hard to use\n",
    "    'zipcode', # too much different values\n",
    "    'bed_type', # noisy and unbalanced\n",
    "    'square_feet', # too many n/a\n",
    "    'experiences_offered', #3 not none\n",
    "    # 'host_response_rate', # -> [0, 1]\n",
    "    'neighbourhood_cleansed', # OHE\n",
    "    'property_type', # OHE + bucketing (b)\n",
    "    'extra_people', #wtf distrib\n",
    "    'host_id', # used\n",
    "    'amenities', # used\n",
    "]\n",
    "\n",
    "tf_feats = [\n",
    "    'host_is_superhost', # tf\n",
    "    'host_identity_verified', # tf\n",
    "    'is_location_exact', # tf\n",
    "    'require_guest_phone_verification', # DISBALANCED\n",
    "    'require_guest_profile_picture', # DISBALANCED\n",
    "    'host_has_profile_pic', # DISBALANCED\n",
    "]\n",
    "\n",
    "tf_is_exist_feats = [\n",
    "    'host_about',\n",
    "    'neighborhood_overview',\n",
    "    'notes',\n",
    "    'transit',\n",
    "]\n",
    "\n",
    "to_bucket_feats = [\n",
    "    'bathrooms', # b\n",
    "    'bedrooms', # b\n",
    "    'beds', # ...\n",
    "    'guests_included',\n",
    "]\n",
    "\n",
    "to_dummy_feats = [\n",
    "    'room_type',\n",
    "    'cancellation_policy',\n",
    "    'host_response_time', # OR binarize?\n",
    "]\n",
    "\n",
    "fill_with_median = [\n",
    "    'security_deposit',\n",
    "    'cleaning_fee',\n",
    "]"
   ]
  },
  {
   "source": [
    "Далее идет самописный класс для преобразования признаков. Изначально он скармливался в pipeline, но поскольку test известен, впоследствии преобразование применялось ко всему датасету."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealtyDataTransform:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # делаем encoding для dummy признаков\n",
    "        df = pd.get_dummies(df, columns=to_dummy_feats)\n",
    "\n",
    "        # Бинаризуем \n",
    "        for col in tf_feats:\n",
    "            df[col] = tf_binarize(df[col])\n",
    "\n",
    "        # Бинаризуем по принципу пустое поле или нет\n",
    "        for col in tf_is_exist_feats:\n",
    "            df[col] = exist_binarize(df[col])\n",
    "\n",
    "        # преобразуем дату, чтобы затем отскалировать ее \n",
    "        # (по сути, признак отвечает за длительность нахождения пользователя на сайте)\n",
    "        # если данных нет, считаем что пользователь новый\n",
    "        df.host_since.astype('datetime64[ns]')\n",
    "        df.host_since = date_transform(df.host_since.astype('datetime64[ns]'))\n",
    "        df.host_since.fillna(df.host_since.max(), inplace=True)\n",
    "\n",
    "        # дополняем данными из других источников\n",
    "        df = pd.merge(df, av_year, on = 'id', how='left')\n",
    "        df = pd.merge(df, unique_comments, on = 'id', how='left')\n",
    "        df = pd.merge(df, host_owns, on = 'host_id', how='left')\n",
    "\n",
    "        # https://www.kaggle.com/brittabettendorf/predicting-prices-xgboost-feature-engineering\n",
    "        # здесь взял идею, как преобразовать столбец `amenities`\n",
    "        df['Laptop_friendly_workspace'] = df['amenities'].str.contains('Laptop friendly workspace').astype(np.int)\n",
    "        df['TV'] = df['amenities'].str.contains('TV').astype(np.int)\n",
    "        df['Family_kid_friendly'] = df['amenities'].str.contains('Family/kid friendly').astype(np.int)\n",
    "        df['Host_greets_you'] = df['amenities'].str.contains('Host greets you').astype(np.int)\n",
    "        df['Smoking_allowed'] = df['amenities'].str.contains('Smoking allowed').astype(np.int)\n",
    "\n",
    "        # Заполним нулями пропуски признака host_response_rate и переведем проценты в дроби\n",
    "        df.host_response_rate.replace(np.NaN, '0', inplace=True)\n",
    "        df.host_response_rate = df.host_response_rate.apply(lambda x: int(x.replace('%','')) / 100)\n",
    "\n",
    "        # убираем то что не требуется для модели\n",
    "        df.drop(drop_features, axis=1, inplace=True)\n",
    "\n",
    "        # заполняем медианой соотв. столбцы\n",
    "        df[fill_with_median] = df[fill_with_median].apply(pd.to_numeric, errors='coerce')\n",
    "        df[fill_with_median] = df[fill_with_median].fillna(df.median())\n",
    "\n",
    "        # все что осталось пустым, заполним минимумом по признаку\n",
    "        df.fillna(df.min(), inplace=True)\n",
    "        return df"
   ]
  },
  {
   "source": [
    "## 3. Build datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Уберем выбросы из трейна\n",
    "train = train[train.price > 0]\n",
    "# train = train[train.price < 700] (работало в худшую сторону)\n",
    "y = train.price\n",
    "train.drop(['price'], axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "Преобразуем данные. Теперь X -набор тренировочных данных."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = RealtyDataTransform()\n",
    "scaler = MinMaxScaler()\n",
    "# pipe = Pipeline(steps=[('feature_engineering', fe), ('scaler' , scaler), ('model', linear_model.LinearRegression())])\n",
    "\n",
    "scaler.fit(fe.transform(df.drop(['price'], axis=1)))\n",
    "X = scaler.transform(fe.transform(train))\n",
    "test = scaler.transform(fe.transform(test))\n",
    "del train, df"
   ]
  },
  {
   "source": [
    "## 4. Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Выделим валидационную выборку."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.to_numpy(), test_size=0.25, random_state=42)"
   ]
  },
  {
   "source": [
    "Будем использовать градиентный бустинг пакета LightGBM."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " import lightgbm as lgbm\n",
    " params = {\n",
    "     'boosting_type': 'gbdt',\n",
    "     'objective': 'mape',\n",
    "     'num_leaves' : 1023,\n",
    "     'learning_rate' : 0.01,\n",
    "     'n_estimators': 2500,\n",
    "     'bagging_fraction' : 0.7,\n",
    "     'feature_fraction' : 0.7,\n",
    " }\n",
    " lgb = lgbm.LGBMRegressor(**params)"
   ]
  },
  {
   "source": [
    "Посмотрим качество модели обученной на X_train."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22.296522562361734"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "lgb.fit(X_train, np.log(y_train))\n",
    "y_pred_lgb = lgb.predict(X_test)\n",
    "mean_absolute_percentage_error(y_test, np.exp(y_pred_lgb))"
   ]
  },
  {
   "source": [
    "Улучшим модель средствами кросс-валидации."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fold MAPE:  23.07836020837106\n",
      "Fold # 1 has processed.\n",
      "Fold MAPE:  23.024808256310124\n",
      "Fold # 2 has processed.\n",
      "Fold MAPE:  23.914602240496702\n",
      "Fold # 3 has processed.\n",
      "Fold MAPE:  22.71684207629346\n",
      "Fold # 4 has processed.\n",
      "Fold MAPE:  23.61185078798279\n",
      "Fold # 5 has processed.\n",
      "Valid prediction:  22.41377424620599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "model = lgbm.LGBMRegressor(**params)\n",
    "\n",
    "valid_ans = np.zeros(y_test.shape[0])\n",
    "test_ans = np.zeros(test.shape[0])\n",
    "current_fold = 0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    current_fold += 1\n",
    "    X_train_cv, X_test_cv = X_train[train_index], X_train[test_index]\n",
    "    y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]\n",
    "    model.fit(X_train_cv, np.log(y_train_cv))\n",
    "    y_pred_fold = model.predict(X_test_cv)\n",
    "    print(\"Fold MAPE: \", mean_absolute_percentage_error(y_test_cv, np.exp(y_pred_fold)))\n",
    "    print(\"Fold #\", current_fold, \"has processed.\")\n",
    "    valid_ans += model.predict(X_test)\n",
    "    test_ans += model.predict(test)\n",
    "print(\"Valid prediction: \", mean_absolute_percentage_error(y_test, np.exp(valid_ans / n_splits)))\n",
    "# формируем итоговое предсказание\n",
    "test_preds = np.exp(test_ans / n_splits)"
   ]
  },
  {
   "source": [
    "## 5. Create a submission"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Собираем итоговый сабмит."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id       price\n",
       "0   9554   36.398165\n",
       "1  11076   52.375804\n",
       "2  13913   46.244453\n",
       "3  17402  249.368863\n",
       "4  24328  115.143515"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9554</td>\n      <td>36.398165</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11076</td>\n      <td>52.375804</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13913</td>\n      <td>46.244453</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17402</td>\n      <td>249.368863</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24328</td>\n      <td>115.143515</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "sub_df = pd.read_csv('sample_submission.csv')\n",
    "sub_df['price'] = test_preds\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv',index=None)"
   ]
  },
  {
   "source": [
    "## 6. Conclusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Итоговый скор на паблике: 23,56 (strong baseline - 24,63)\n",
    "\n",
    "Итоговый скор на прайвате: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Что не сработало: \n",
    "- во многих работах по похожим датасетам хорошо работало извлечение размера помещения (с предсказанием пропусков, например, линейной моделью), однако почечему-то у меня это не дало никакого прироста;\n",
    "- использование расстояния до центра города также не дало никакого преимущества над обычными координатами;\n",
    "- другие фичи по данным из Calendar, Reviews (меньшие временнЫе окна, например);\n",
    "\n",
    "На что не хватило времени:\n",
    "- фактически всё NLP (например, классификация отзывов по удовлетворенности сторонней моделью, затем составление рейтинга лота);\n",
    "- другие приложения геоданных (ближайшие ТЦ, исторические объекты, удаленность от города)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Спасибо за внимание!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}